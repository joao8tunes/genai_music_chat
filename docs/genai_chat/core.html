<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>genai_chat.core API documentation</title>
<meta name="description" content="Core GenAI operations." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>genai_chat.core</code></h1>
</header>
<section id="section-intro">
<p>Core GenAI operations.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# encoding: utf-8

# JoÃ£o Antunes &lt;joao8tunes@gmail.com&gt;
# https://github.com/joao8tunes

&#34;&#34;&#34;
Core GenAI operations.
&#34;&#34;&#34;

import logging
import json
import re
import os

from genai_chat.utils import compare_strings
from genai_chat.settings import get_settings


class Bot:
    &#34;&#34;&#34;
    Abstract GenAI bot.
    &#34;&#34;&#34;

    _genai_name: str
    &#34;&#34;&#34;Generative AI technology name.&#34;&#34;&#34;
    _prompt_behavior: str
    &#34;&#34;&#34;Bot behavior: initial prompt.&#34;&#34;&#34;
    _citation_regex: str
    &#34;&#34;&#34;Regex used to fetch citations from bot message.&#34;&#34;&#34;
    _citation_threshold: float
    &#34;&#34;&#34;Threshold for fuzzy citation search, between 0 and 1.&#34;&#34;&#34;
    _citation_field: str
    &#34;&#34;&#34;Field used to index citations.&#34;&#34;&#34;
    _citations_available: [dict]
    &#34;&#34;&#34;List of external data.&#34;&#34;&#34;
    _database_filepath: str
    &#34;&#34;&#34;Database filepath with external data.&#34;&#34;&#34;
    _filter_bot_messages_without_citations: bool
    &#34;&#34;&#34;Whether to filter bot messages with no citations.&#34;&#34;&#34;
    _error_message_bot_message_without_citations: str
    &#34;&#34;&#34;Default message shown when bot messages has no citations.&#34;&#34;&#34;
    _error_message_general: str
    &#34;&#34;&#34;Default error message.&#34;&#34;&#34;
    _chat_history: [dict]
    &#34;&#34;&#34;Chat history.&#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;
        Instantiates an abstract GenAI bot object.
        &#34;&#34;&#34;
        genai_settings = get_settings().get(&#39;genai_chat&#39;)

        self._prompt_behavior = genai_settings.get(&#39;prompt_behavior&#39;, &#34;&#34;)
        self._citation_regex = genai_settings.get(&#39;citation_regex&#39;, r&#39;&#34;([^&#34;]*)&#34;&#39;)
        self._citation_threshold = genai_settings.get(&#39;citation_threshold&#39;, 0.75)
        self._citation_field = genai_settings.get(&#39;citation_field&#39;, &#34;title&#34;)
        self._citations_available = []
        self._chat_history = []

        database_filepath_rel = genai_settings.get(&#39;database_filepath&#39;, &#34;assets/database.json&#34;)
        this_dir_path = os.path.abspath(os.path.join(os.path.realpath(__file__), os.pardir))
        self._database_filepath = os.path.join(*[this_dir_path, &#34;..&#34;, database_filepath_rel])

        self._filter_bot_messages_without_citations = \
            genai_settings.get(&#39;filter_bot_messages_without_citations&#39;, True)
        self._error_message_bot_message_without_citations = \
            genai_settings.get(&#39;error_message_bot_message_without_citations&#39;, &#34;&#34;)
        self._error_message_general = genai_settings.get(&#39;error_message_general&#39;, &#34;&#34;)

    def get_genai_name(self) -&gt; str:
        &#34;&#34;&#34;
        Get the Generative AI technology name.

        Returns
        -------
        _genai_name: str
            Generative AI technology name.
        &#34;&#34;&#34;
        return self._genai_name

    def _load_database(self) -&gt; [dict]:
        &#34;&#34;&#34;
        Load a database from a JSON file.

        Returns
        -------
        data: [dict]
            Database.
        &#34;&#34;&#34;
        with open(self._database_filepath, mode=&#34;rt&#34;, encoding=&#34;utf-8&#34;) as file:
            data = json.load(file)

        return data

    def _extract_citations(self, bot_message: str) -&gt; [dict]:
        &#34;&#34;&#34;
        Extracts the citations contained in the LLM bot message using fuzzy string match.

        Parameters
        ----------
        bot_message: str
            Message from LLM bot.

        Returns
        -------
        citations_list: [dict]
            List of citations.
        &#34;&#34;&#34;
        logging.debug(&#34;Extracting citations from message...&#34;)
        selected_citations = {}
        citations = self._citations_available.copy()

        if citations:
            # Extracting citations using regex and fuzzy string matching:
            names_list = re.findall(pattern=self._citation_regex, string=bot_message)

            if names_list:
                for name in names_list:
                    max_similarity = 0.0
                    ref, ref_index = None, None

                    for data_index, data in enumerate(citations, start=0):
                        similarity = compare_strings(name, data[self._citation_field], case_sensitive=False)

                        if similarity &gt; max_similarity:
                            max_similarity = similarity
                            ref, ref_index = data, data_index

                    if max_similarity &gt;= self._citation_threshold:
                        selected_citations[ref_index] = ref

            # Extracting citations using only fuzzy string matching:
            for data_index, data in enumerate(citations, start=0):
                if data_index not in selected_citations:
                    similarity = compare_strings(
                        data[self._citation_field],
                        bot_message,
                        fuzzy_method=&#34;partial_ratio&#34;,
                        case_sensitive=False
                    )

                    if similarity &gt;= self._citation_threshold:
                        selected_citations[data_index] = data.copy()

        if selected_citations:
            citations, citations_ids = list(selected_citations.values()), list(selected_citations.keys())
            logging.debug(f&#34;Selected citations: {citations_ids}&#34;)
        else:
            logging.warning(&#34;No citations found.&#34;)
            citations = []

        return citations

    def chat(self, user_message: str, **kwargs) -&gt; (str, [dict]):
        &#34;&#34;&#34;
        Abstract chat method: sends a user message and receives a bot response from a LLM.

        Parameters
        ----------
        user_message: str
            User message.
        **kwargs: dict
            Keyword-based arguments.

        Returns
        -------
        bot_message, bot_citations: (str, [dict])
            Bot message and list os citations.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def _add_user_message(self, user_message: str) -&gt; None:
        &#34;&#34;&#34;
        Add user message to chat history.

        Parameters
        ----------
        user_message: str
            User message.
        &#34;&#34;&#34;
        self._chat_history.append(
            {
                &#34;author&#34;: &#34;user&#34;,
                &#34;message&#34;: user_message
            }
        )

    def _add_assistant_message(self, assistant_message: str, assistant_citations: [dict]) -&gt; None:
        &#34;&#34;&#34;
        Add assistant message to chat history.

        Parameters
        ----------
        assistant_message: str
            Assistant message.
        assistant_citations: [dict]
            Assistant citations.
        &#34;&#34;&#34;
        self._chat_history.append(
            {
                &#34;author&#34;: &#34;assistant&#34;,
                &#34;message&#34;: assistant_message,
                &#34;citations&#34;: assistant_citations
            }
        )

    def get_chat_history(self) -&gt; [dict]:
        &#34;&#34;&#34;
        Get chat history.

        Returns
        -------
        _chat_history: [dict]
            Chat history.
        &#34;&#34;&#34;
        return self._chat_history

    def get_music_recommendations(
            self,
            title: str = None,
            genre: str = None,
            authors: str = None,
            country: str = None,
            year: int = None
    ) -&gt; [dict]:
        &#34;&#34;&#34;
        Get music recommendations from database.

        Parameters
        ----------
        title: str
            Music title.
        genre: str
            Music genre.
        authors: str
            Music authors.
        country: str
            Music country.
        year: int
            Music year.

        Returns
        -------
        data: [dict]
            List of music recommendations.

        Notes
        -----
        This method allows for the incorporation of external data into the LLM without requiring model retraining. This
        integration is useful in refining the information considered by the LLM, leading to more personalized responses.
        Given that GenAI charges for LLM usage per token/word, it&#39;s prudent to limit the amount of external data
        injected into the LLM per request.

        It&#39;s recommended to adopt an approach that provides the user with the top N options based on identified
        parameters, rather than loading the entire database into the LLM. Additionally, you have the option to employ a
        custom recommendation engine or utilize a semantic search service like Azure Cognitive Search or GCP Cloud
        Search (also known as GCP Enterprise Search).
        &#34;&#34;&#34;
        data = []

        try:
            logging.debug(&#34;Fetching data using simple search...&#34;)
            search_params = {&#34;title&#34;: title, &#34;genre&#34;: genre, &#34;authors&#34;: authors, &#34;country&#34;: country, &#34;year&#34;: year}
            logging.debug(f&#34;Search parameters: {search_params}&#34;)
            data = self._load_database()
            logging.debug(f&#34;Fetched {len(data)} results.&#34;)
        except Exception as e:
            logging.warning(f&#34;Failed to fetch data: {e}&#34;)
            pass

        return data</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="genai_chat.core.Bot"><code class="flex name class">
<span>class <span class="ident">Bot</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract GenAI bot.</p>
<p>Instantiates an abstract GenAI bot object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Bot:
    &#34;&#34;&#34;
    Abstract GenAI bot.
    &#34;&#34;&#34;

    _genai_name: str
    &#34;&#34;&#34;Generative AI technology name.&#34;&#34;&#34;
    _prompt_behavior: str
    &#34;&#34;&#34;Bot behavior: initial prompt.&#34;&#34;&#34;
    _citation_regex: str
    &#34;&#34;&#34;Regex used to fetch citations from bot message.&#34;&#34;&#34;
    _citation_threshold: float
    &#34;&#34;&#34;Threshold for fuzzy citation search, between 0 and 1.&#34;&#34;&#34;
    _citation_field: str
    &#34;&#34;&#34;Field used to index citations.&#34;&#34;&#34;
    _citations_available: [dict]
    &#34;&#34;&#34;List of external data.&#34;&#34;&#34;
    _database_filepath: str
    &#34;&#34;&#34;Database filepath with external data.&#34;&#34;&#34;
    _filter_bot_messages_without_citations: bool
    &#34;&#34;&#34;Whether to filter bot messages with no citations.&#34;&#34;&#34;
    _error_message_bot_message_without_citations: str
    &#34;&#34;&#34;Default message shown when bot messages has no citations.&#34;&#34;&#34;
    _error_message_general: str
    &#34;&#34;&#34;Default error message.&#34;&#34;&#34;
    _chat_history: [dict]
    &#34;&#34;&#34;Chat history.&#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;
        Instantiates an abstract GenAI bot object.
        &#34;&#34;&#34;
        genai_settings = get_settings().get(&#39;genai_chat&#39;)

        self._prompt_behavior = genai_settings.get(&#39;prompt_behavior&#39;, &#34;&#34;)
        self._citation_regex = genai_settings.get(&#39;citation_regex&#39;, r&#39;&#34;([^&#34;]*)&#34;&#39;)
        self._citation_threshold = genai_settings.get(&#39;citation_threshold&#39;, 0.75)
        self._citation_field = genai_settings.get(&#39;citation_field&#39;, &#34;title&#34;)
        self._citations_available = []
        self._chat_history = []

        database_filepath_rel = genai_settings.get(&#39;database_filepath&#39;, &#34;assets/database.json&#34;)
        this_dir_path = os.path.abspath(os.path.join(os.path.realpath(__file__), os.pardir))
        self._database_filepath = os.path.join(*[this_dir_path, &#34;..&#34;, database_filepath_rel])

        self._filter_bot_messages_without_citations = \
            genai_settings.get(&#39;filter_bot_messages_without_citations&#39;, True)
        self._error_message_bot_message_without_citations = \
            genai_settings.get(&#39;error_message_bot_message_without_citations&#39;, &#34;&#34;)
        self._error_message_general = genai_settings.get(&#39;error_message_general&#39;, &#34;&#34;)

    def get_genai_name(self) -&gt; str:
        &#34;&#34;&#34;
        Get the Generative AI technology name.

        Returns
        -------
        _genai_name: str
            Generative AI technology name.
        &#34;&#34;&#34;
        return self._genai_name

    def _load_database(self) -&gt; [dict]:
        &#34;&#34;&#34;
        Load a database from a JSON file.

        Returns
        -------
        data: [dict]
            Database.
        &#34;&#34;&#34;
        with open(self._database_filepath, mode=&#34;rt&#34;, encoding=&#34;utf-8&#34;) as file:
            data = json.load(file)

        return data

    def _extract_citations(self, bot_message: str) -&gt; [dict]:
        &#34;&#34;&#34;
        Extracts the citations contained in the LLM bot message using fuzzy string match.

        Parameters
        ----------
        bot_message: str
            Message from LLM bot.

        Returns
        -------
        citations_list: [dict]
            List of citations.
        &#34;&#34;&#34;
        logging.debug(&#34;Extracting citations from message...&#34;)
        selected_citations = {}
        citations = self._citations_available.copy()

        if citations:
            # Extracting citations using regex and fuzzy string matching:
            names_list = re.findall(pattern=self._citation_regex, string=bot_message)

            if names_list:
                for name in names_list:
                    max_similarity = 0.0
                    ref, ref_index = None, None

                    for data_index, data in enumerate(citations, start=0):
                        similarity = compare_strings(name, data[self._citation_field], case_sensitive=False)

                        if similarity &gt; max_similarity:
                            max_similarity = similarity
                            ref, ref_index = data, data_index

                    if max_similarity &gt;= self._citation_threshold:
                        selected_citations[ref_index] = ref

            # Extracting citations using only fuzzy string matching:
            for data_index, data in enumerate(citations, start=0):
                if data_index not in selected_citations:
                    similarity = compare_strings(
                        data[self._citation_field],
                        bot_message,
                        fuzzy_method=&#34;partial_ratio&#34;,
                        case_sensitive=False
                    )

                    if similarity &gt;= self._citation_threshold:
                        selected_citations[data_index] = data.copy()

        if selected_citations:
            citations, citations_ids = list(selected_citations.values()), list(selected_citations.keys())
            logging.debug(f&#34;Selected citations: {citations_ids}&#34;)
        else:
            logging.warning(&#34;No citations found.&#34;)
            citations = []

        return citations

    def chat(self, user_message: str, **kwargs) -&gt; (str, [dict]):
        &#34;&#34;&#34;
        Abstract chat method: sends a user message and receives a bot response from a LLM.

        Parameters
        ----------
        user_message: str
            User message.
        **kwargs: dict
            Keyword-based arguments.

        Returns
        -------
        bot_message, bot_citations: (str, [dict])
            Bot message and list os citations.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def _add_user_message(self, user_message: str) -&gt; None:
        &#34;&#34;&#34;
        Add user message to chat history.

        Parameters
        ----------
        user_message: str
            User message.
        &#34;&#34;&#34;
        self._chat_history.append(
            {
                &#34;author&#34;: &#34;user&#34;,
                &#34;message&#34;: user_message
            }
        )

    def _add_assistant_message(self, assistant_message: str, assistant_citations: [dict]) -&gt; None:
        &#34;&#34;&#34;
        Add assistant message to chat history.

        Parameters
        ----------
        assistant_message: str
            Assistant message.
        assistant_citations: [dict]
            Assistant citations.
        &#34;&#34;&#34;
        self._chat_history.append(
            {
                &#34;author&#34;: &#34;assistant&#34;,
                &#34;message&#34;: assistant_message,
                &#34;citations&#34;: assistant_citations
            }
        )

    def get_chat_history(self) -&gt; [dict]:
        &#34;&#34;&#34;
        Get chat history.

        Returns
        -------
        _chat_history: [dict]
            Chat history.
        &#34;&#34;&#34;
        return self._chat_history

    def get_music_recommendations(
            self,
            title: str = None,
            genre: str = None,
            authors: str = None,
            country: str = None,
            year: int = None
    ) -&gt; [dict]:
        &#34;&#34;&#34;
        Get music recommendations from database.

        Parameters
        ----------
        title: str
            Music title.
        genre: str
            Music genre.
        authors: str
            Music authors.
        country: str
            Music country.
        year: int
            Music year.

        Returns
        -------
        data: [dict]
            List of music recommendations.

        Notes
        -----
        This method allows for the incorporation of external data into the LLM without requiring model retraining. This
        integration is useful in refining the information considered by the LLM, leading to more personalized responses.
        Given that GenAI charges for LLM usage per token/word, it&#39;s prudent to limit the amount of external data
        injected into the LLM per request.

        It&#39;s recommended to adopt an approach that provides the user with the top N options based on identified
        parameters, rather than loading the entire database into the LLM. Additionally, you have the option to employ a
        custom recommendation engine or utilize a semantic search service like Azure Cognitive Search or GCP Cloud
        Search (also known as GCP Enterprise Search).
        &#34;&#34;&#34;
        data = []

        try:
            logging.debug(&#34;Fetching data using simple search...&#34;)
            search_params = {&#34;title&#34;: title, &#34;genre&#34;: genre, &#34;authors&#34;: authors, &#34;country&#34;: country, &#34;year&#34;: year}
            logging.debug(f&#34;Search parameters: {search_params}&#34;)
            data = self._load_database()
            logging.debug(f&#34;Fetched {len(data)} results.&#34;)
        except Exception as e:
            logging.warning(f&#34;Failed to fetch data: {e}&#34;)
            pass

        return data</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="genai_chat.openai.BotOpenAI" href="openai.html#genai_chat.openai.BotOpenAI">BotOpenAI</a></li>
<li><a title="genai_chat.palm.BotPaLM" href="palm.html#genai_chat.palm.BotPaLM">BotPaLM</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="genai_chat.core.Bot.get_genai_name"><code class="name flex">
<span>def <span class="ident">get_genai_name</span></span>(<span>self) â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Get the Generative AI technology name.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_genai_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Generative AI technology name.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_genai_name(self) -&gt; str:
    &#34;&#34;&#34;
    Get the Generative AI technology name.

    Returns
    -------
    _genai_name: str
        Generative AI technology name.
    &#34;&#34;&#34;
    return self._genai_name</code></pre>
</details>
</dd>
<dt id="genai_chat.core.Bot.chat"><code class="name flex">
<span>def <span class="ident">chat</span></span>(<span>self, user_message:Â str, **kwargs) â€‘>Â (<classÂ 'str'>,Â [<classÂ 'dict'>])</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract chat method: sends a user message and receives a bot response from a LLM.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>user_message</code></strong> :&ensp;<code>str</code></dt>
<dd>User message.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Keyword-based arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bot_message</code></strong>, <strong><code>bot_citations</code></strong> :&ensp;<code>(str, [dict])</code></dt>
<dd>Bot message and list os citations.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def chat(self, user_message: str, **kwargs) -&gt; (str, [dict]):
    &#34;&#34;&#34;
    Abstract chat method: sends a user message and receives a bot response from a LLM.

    Parameters
    ----------
    user_message: str
        User message.
    **kwargs: dict
        Keyword-based arguments.

    Returns
    -------
    bot_message, bot_citations: (str, [dict])
        Bot message and list os citations.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="genai_chat.core.Bot.get_chat_history"><code class="name flex">
<span>def <span class="ident">get_chat_history</span></span>(<span>self) â€‘>Â [<classÂ 'dict'>]</span>
</code></dt>
<dd>
<div class="desc"><p>Get chat history.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>_chat_history</code></strong> :&ensp;<code>[dict]</code></dt>
<dd>Chat history.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_chat_history(self) -&gt; [dict]:
    &#34;&#34;&#34;
    Get chat history.

    Returns
    -------
    _chat_history: [dict]
        Chat history.
    &#34;&#34;&#34;
    return self._chat_history</code></pre>
</details>
</dd>
<dt id="genai_chat.core.Bot.get_music_recommendations"><code class="name flex">
<span>def <span class="ident">get_music_recommendations</span></span>(<span>self, title:Â strÂ =Â None, genre:Â strÂ =Â None, authors:Â strÂ =Â None, country:Â strÂ =Â None, year:Â intÂ =Â None) â€‘>Â [<classÂ 'dict'>]</span>
</code></dt>
<dd>
<div class="desc"><p>Get music recommendations from database.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code></dt>
<dd>Music title.</dd>
<dt><strong><code>genre</code></strong> :&ensp;<code>str</code></dt>
<dd>Music genre.</dd>
<dt><strong><code>authors</code></strong> :&ensp;<code>str</code></dt>
<dd>Music authors.</dd>
<dt><strong><code>country</code></strong> :&ensp;<code>str</code></dt>
<dd>Music country.</dd>
<dt><strong><code>year</code></strong> :&ensp;<code>int</code></dt>
<dd>Music year.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>[dict]</code></dt>
<dd>List of music recommendations.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>This method allows for the incorporation of external data into the LLM without requiring model retraining. This
integration is useful in refining the information considered by the LLM, leading to more personalized responses.
Given that GenAI charges for LLM usage per token/word, it's prudent to limit the amount of external data
injected into the LLM per request.</p>
<p>It's recommended to adopt an approach that provides the user with the top N options based on identified
parameters, rather than loading the entire database into the LLM. Additionally, you have the option to employ a
custom recommendation engine or utilize a semantic search service like Azure Cognitive Search or GCP Cloud
Search (also known as GCP Enterprise Search).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_music_recommendations(
        self,
        title: str = None,
        genre: str = None,
        authors: str = None,
        country: str = None,
        year: int = None
) -&gt; [dict]:
    &#34;&#34;&#34;
    Get music recommendations from database.

    Parameters
    ----------
    title: str
        Music title.
    genre: str
        Music genre.
    authors: str
        Music authors.
    country: str
        Music country.
    year: int
        Music year.

    Returns
    -------
    data: [dict]
        List of music recommendations.

    Notes
    -----
    This method allows for the incorporation of external data into the LLM without requiring model retraining. This
    integration is useful in refining the information considered by the LLM, leading to more personalized responses.
    Given that GenAI charges for LLM usage per token/word, it&#39;s prudent to limit the amount of external data
    injected into the LLM per request.

    It&#39;s recommended to adopt an approach that provides the user with the top N options based on identified
    parameters, rather than loading the entire database into the LLM. Additionally, you have the option to employ a
    custom recommendation engine or utilize a semantic search service like Azure Cognitive Search or GCP Cloud
    Search (also known as GCP Enterprise Search).
    &#34;&#34;&#34;
    data = []

    try:
        logging.debug(&#34;Fetching data using simple search...&#34;)
        search_params = {&#34;title&#34;: title, &#34;genre&#34;: genre, &#34;authors&#34;: authors, &#34;country&#34;: country, &#34;year&#34;: year}
        logging.debug(f&#34;Search parameters: {search_params}&#34;)
        data = self._load_database()
        logging.debug(f&#34;Fetched {len(data)} results.&#34;)
    except Exception as e:
        logging.warning(f&#34;Failed to fetch data: {e}&#34;)
        pass

    return data</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="genai_chat" href="index.html">genai_chat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="genai_chat.core.Bot" href="#genai_chat.core.Bot">Bot</a></code></h4>
<ul class="">
<li><code><a title="genai_chat.core.Bot.get_genai_name" href="#genai_chat.core.Bot.get_genai_name">get_genai_name</a></code></li>
<li><code><a title="genai_chat.core.Bot.chat" href="#genai_chat.core.Bot.chat">chat</a></code></li>
<li><code><a title="genai_chat.core.Bot.get_chat_history" href="#genai_chat.core.Bot.get_chat_history">get_chat_history</a></code></li>
<li><code><a title="genai_chat.core.Bot.get_music_recommendations" href="#genai_chat.core.Bot.get_music_recommendations">get_music_recommendations</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>